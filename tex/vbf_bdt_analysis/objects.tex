
\subsection{Electrons}
\label{chap:analysis:sec:objects:subsec:electrons}

With two leptons in the final state, it is crucial to efficiently
select leptons, while minimizing contamination from backgrounds with
jets faking leptons. Such backgrounds are \wjets and multijet production
via QCD. Because these backgrounds fall off quickly with increasing
lepton \pt, lepton selections have been optimized in bins of \pt. For
electron identification, the likelihood-based identification category
\textsc{very tight}
(section~\ref{chap:reco:section:electron:subsec:ident}), which has the
highest background-rejecting power of all of the identification
categories, is required at low electron \et~($\et < 25 \gev$). To
recover efficiency at high \et, where the fake background
contribution is smaller, the identification category is relaxed to
cut-based {\it medium}. The standard {\it medium} definition is
modified slightly to improve rejection against electrons from photon
conversions. Electrons are required to have a hit in the pixel b-layer
if one is expected given the track parameters, and the electron should
not be flagged as a conversion candidate by the reconstruction
software (need to find reference for conversion bit). 

Further rejection of fake background is accomplished by requiring
electrons to be isolated. Cuts are placed on the ratio of the sum of
the \et~of topological clusters in a
cone of radius $R$ around the electron to the electron \et,
$E_{R=r}/\et$, where $r$ and the cut value have been optimized in bins
of \et. To avoid including the energy from the electron itself, the
energy in a window of dimension $\Delta \eta \times \Delta \phi =
0.125 \times 0.175$ centered on the electron candidate is removed from
the isolation energy sum. An additional correction to the isolation
energy is applied on an event-by-event basis to account for
calorimeter energy arising from pileup and the underlying
event~\cite{bib:Cacciari:2007fd}.

In addition to this calorimeter-based
isolation cut, electrons are required to satisfy track-based isolation
requirements in which
the \pt~sum of the tracks in a cone of radius $R$ divided by the
electron \et~is the discrimating variable. Track-based isolation is
more robust against pileup, making it a more powerful discriminant
against fakes. Finally, cuts are placed the transverse
electron impact parameter significance ($d_0/\sigma (d_0) < 3$), as well as the
longitudinal impact parameter ($z_0\sin(\theta) < 0.4$ mm), for
rejection against fake electrons arising from pileup
vertices. Table~\ref{chap:analysis:tab:electron_selection} summarizes
the electron selection.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\hline
  \et &  & & & impact \\
 (\gev) & electron ID  & calo.~isolation & track isolation &
  parameters \\
\hline
10-15 & \multirow{3}{0.18\textwidth}{\centering \textsc{very tight} likelihood} &
  $\et^{R=0.3}/\et<0.20$ & $p_T^{R=0.4}/\et<0.06$
  & \multirow{5}{0.22\textwidth}{\centering $d_0/\sigma{(d_0)} < 3.0$,
  $z_0 \sin{\theta}<0.4$ mm} \\
   \cline{1-1}\cline{3-4}
15-20 & & $\et^{R=0.3}/\et<0.24$ & $p_T^{R=0.3}/\et<0.08$ & \\
   \cline{1-1}\cline{3-4}
20-25 & & \multirow{3}{*}{$\et^{R=0.3}/\et<0.28$} &
\multirow{3}{*}{$p_T^{R=0.3}/\et<0.10$} & \\
   \cline{1-2}
\multirow{2}{*}{$>25$} & {\it medium} &  & \\
       & $+$ conversion &  & \\
\hline
\end{tabular}
}
\caption[Electron selection summary in \et~bins.]{Electron selection summary in
  \et~bins, including the identification category, the calorimeter and
track isolation cuts, and the impact parameter cuts.}
\label{chap:analysis:tab:electron_selection}
\end{table}

To account for mismodeling of electron variables in simulation,
scale factors (SF) are applied to each electron selected in the MC
prediction
(section~\ref{chap:reco:section:electron:subsec:ident}). These SFs
correct the efficiency differences between data and simulation for the
trigger, reconstruction, identification, isolation, and impact
parameter requirements. All of these efficiencies are measured with an
electron-rich $Z\rightarrow{ee}$ sample in data with the tag-and-probe
technique described in chapter~\ref{chap:reco}. The total electron
efficiency is shown in table~\ref{chap:analysis:tab:lepton_eff} in
bins of \et~for a \hww sample at $m_H = 125 \gev$. Due to the use of \textsc{very tight} identification for
$\et < 25 \gev$, the efficiency in the low \et~bins is relatively low,
ranging from 40\% to 70\%. 

\begin{table}[h]
\centering
\begin{tabular}{l|c|c}
\hline
$E_{T}$ & $\epsilon_{\textrm{electron}}$ & $\epsilon_{\textrm{muon}}$
\\ \hline
10-15 & $0.412 \pm 0.016 \pm 0.016$ & $0.574 \pm 0.027 \pm 0.005$ \\
15-20 & $0.619 \pm 0.009 \pm 0.024$ & $0.808 \pm 0.012 \pm 0.005$ \\
20-25 & $0.668 \pm 0.008 \pm 0.027$ & $0.904 \pm 0.007 \pm 0.005$ \\
25-30 & $0.755 \pm 0.007 \pm 0.014$ & $0.924 \pm 0.006 \pm 0.005$ \\
30-35 & $0.770 \pm 0.007 \pm 0.005$ & $0.932 \pm 0.006 \pm 0.005$ \\
35-40 & $0.796 \pm 0.006 \pm 0.003$ & $0.942 \pm 0.005 \pm 0.005$ \\
40-45 & $0.798 \pm 0.006 \pm 0.002$ & $0.943 \pm 0.005 \pm 0.005$ \\
45-50 & $0.813 \pm 0.006 \pm 0.002$ & $0.944 \pm 0.005 \pm 0.005$ \\
\hline
\end{tabular}
\caption[Total lepton selection efficiencies and associated
  relative uncertainties.]{Total lepton selection efficiencies and associated
  relative uncertainties for a $m_H= 125 \gev$ Higgs signal
  sample. The uncertainties are split into isolation and
  reconstruction/identification, respectively. If the relative
  uncertainty is less than 0.005, then 0.005 is shown. The total
  uncertainty can be obtained by adding the two
  components in quadrature.}
\label{chap:analysis:tab:lepton_eff}
\end{table}

\subsection{Muons}

This analysis requires muons to be of the ``combined'' type in which
the combined muon track is a statistical combination of independent ID and MS
track fits (section~\ref{chap:reco:sec:muon}). The ID track associated
with the muon must satisfy the requirements: (1) the sum of pixel hits
and dead pixel sensors crossed by the track must be greater than zero,
(2) the sum of SCT hits and dead SCT sensors cross by the track must
be greater than four, the number of missing hits in a cross sensor
that is not dead must be less than three, and (4) a TRT extension is
found if the track falls within the TRT acceptance. These cuts have
been studied rigorously by a dedicated performance group that also
computes the SFs associated with such a selection using
tag-and-probe. 

Although muons are less prone than electrons to being faked by jets in $W$+jets and
QCD multijet processes, and by electrons from photon conversions,
requirements are placed on isolation variables as well as
the transverse and longitudinal impact parameters. Isolation cut
values have been optimized in bins of muon \pt. To account for pileup
dependence, the calorimeter isolation is corrected event-by-event
according to \nvtx (show equation?). To determine the
optimal impact parameter cut values, a sensitivity scan has been
performed over the two-dimensional parameter space for the
$d_0/\sigma{(d_0)}$ and $z_0\sin(\theta)$ cuts. The resulting
selection is shown in table~\ref{chap:analysis:tab:muon_selection}.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
  \pt & & & impact \\
 (\gev) & calo.~isolation & track isolation & parameters \\
\hline
10-15 & $\et^{R=0.3}/\pt<0.06$ & $\pt^{R=0.4}/\pt<0.06$
 & \multirow{4}{0.22\textwidth}{\centering $d_0/\sigma{(d_0)} < 3.0$,
  $z_0 \sin{\theta}<1.0$ mm} \\
   \cline{1-3}
15-20 & $\et^{R=0.3}/\pt<0.12$ & $\pt^{R=0.3}/\pt<0.08$ & \\
   \cline{1-3}
20-25 & $\et^{R=0.3}/\pt<0.18$ & \multirow{2}{*}{\centering $p_T^{R=0.3}/\pt<0.12$} & \\
   \cline{1-2}
$>25$ & $\et^{R=0.3}/\pt<0.30$ & & \\
\hline
\end{tabular}
}
\caption[Muon selection summary in \pt~bins.]{Muon selection summary in
  \pt~bins, including the identification category, the calorimeter and
track isolation cuts, and the impact parameter cuts.}
\label{chap:analysis:tab:muon_selection}
\end{table}

Data-derived efficiencies associated with the impact parameter and isolation cuts
are also measured with tag-and-probe and those from simulation are
corrected to reflect data. The resulting total efficiency for
selecting a muon is shown in
table~\ref{chap:analysis:tab:lepton_eff}. Compared to electrons, the
muon efficiencies associated with the optimal cuts are 20\%-40\%
higher, due to the reduced background from jets and photons faking
muons.

\subsection{Jets}

As discussed in section~\ref{chap:reco:sec:jet}, jets are
reconstructed with the \antikt $R=0.4$ algorithm and calibrated with
LCW. These jets are required to satisfy the quality criteria for the
L\textsc{ooser} category, which has the highest efficiency of all of
the jet cleaning categories. The remaining jet cuts are optimized for
maximum sensitivity, while minimizing fake jets from pileup which
induce migrations into and out of the signal regions. In the
optimization, cut values for the jet \pt~and the JVF (defined in
section~\ref{chap:reco:sec:jet:subsec:quality}) are simultaneously
scanned, extracting both an estimate of the expected significance, as
well as the uncertainty due to bin migrations. The optimal
configuration has been found to be jet $\pt > 25 \gev$ and
$\textrm{JVF} > 0.5$ in the $|\eta| < 2.4$ region. In the region
outside of the tracker, $2.4 \leq |\eta| < 4.5$, where JVF is
unavailable, the \pt~threshold is increased to $30 \gev$ to compensate
for the absence of a cut on JVF. Finally, jets in $|\eta| < 2.4$ with
$\pt > 50 \gev$ do not have any JVF requirement, since the rate of
pileup jets in this \pt~range is relatively small, and therefore a JVF
requirement only serves to degrade the efficiency. The resulting
uncertainty due to jet bin migrations for these cuts is 6.3\% in a
VBF-rich region of phase space, significantly smaller than
uncertainties due to JES and JER (section~\ref{chap:reco:sec:jet}). 

The jet definition above defines the tag jets in the VBF topology. In
addition to these jets, another jet collection is considered in the
VBF analysis. Jets which fall in between the pseudorapidity range
defined by the two tag jets with $\pt > 15 \gev$ and which satisfy the
same JVF requirement as the tag jets are placed into a separate jet
collection. This central jet collection is used to veto events with
high \pt~central jets.

\subsection{$b$-hadron Jets}
\label{chap:analysis:sec:objects:subsec:btag}

The tagging of heavy flavor jets ($b$-jets), described in
section~\ref{chap:reco:sec:btag}, is crucial for (1) rejecting
background processes with top quarks, and (2) establishing a control
region from which the normalization of such backgrounds is
extrapolated. In this analysis, $b$-tagging is performed with the MV1
algorithm, a multi-layered neural network approach that incorporates
impact parameter and reconstructed vertex information from tracks
associated with jets. Each jet is assigned a score quantifying the
probability that it is a $b$ jet, and if the score falls over a
threshold, then the jet is tagged. The threshold
has been chosen such that 85\% of true $b$-jets are tagged. 

The associated $b$-tagging efficiencies are measured in a \ttbar-rich
control region in data, using the likelihood procedure described in
section~\ref{chap:reco:sec:btag}. Scale factors, defined as the ratio
of the tagging efficiency in data to that in simulation, are then
applied in simulation. These SFs, which are evaluated in six \pt~bins,
are shown with their associated uncertainties, in
table~\ref{chap:analysis:tab:btag_sfs}. The SFs deviate from unity by
less than 5\% and are consistent with unity within statistical and
systematic uncertainties. 

For the uncertainties on scale factors, the eigenvector method is used
to reduce the number of uncorrelated variations. A covariance matrix
is constructed for each source of uncertainty, and the total
covariance matrix is then obtained by summing each source matrix. The
total covariance matrix is transformed to its eigenbasis, and the
square root of the matrix eigenvalues are then taken as the fully
uncorrelated systematic variations on the $b$-tag SFs. 

In addition to the $b$-tag SFs, there are SFs to correct the mis-tag
efficiency for $c$-jets and light flavor jets. The uncertainties on
these two factors consider the same sources as those for $b$-tag SFs,
but in this case, the uncertainties for each source are summed in
quadrature for a single variation. 

\begin{table}[h]
  \centering
  \begin{tabular}{c || c | c | c | c }
  \hline
  \multirow{2}{*}{\pt~bin} & Scale & Statistical & Systematic & Total
  \\
   & Factor & Uncertainty & Uncertainty & Uncertainty \\
  \hline
  $20-30$ & 0.999 & 1.5\% & 4.7\% & 4.9\% \\
  $30-60$ & 1.006 & 0.5\% & 1.8\% & 1.9\% \\
  $60-90$ & 0.989 & 0.5\% & 1.6\% & 1.6\% \\
  $90-140$ & 0.996 & 0.6\% & 1.4\% & 1.5\% \\
  $140-200$ & 0.965 & 1.2\% & 2.9\% & 3.1\% \\
  $200-300$ & 1.046 & 2.9\% & 6.6\% & 7.2\% \\
  \hline
  \end{tabular}
  \caption[$b$-tagging scale factors and uncertainties in
    \pt~bins.]{$b$-tagging scale factors and uncertainties in
    \pt~bins. Uncertainties are split into statistical and systematic
    components, and the total uncertainty is the sum in quadrature of
    the two.}
  \label{chap:analysis:tab:btag_sfs}
\end{table}

\subsection{\etmiss}

Missing transverse momentum (\etmiss) is discussed at length in
section~\ref{chap:reco:sec:met}. In this analysis, \etmiss
arises due to the presence of undetected neutrinos from the $W$ boson
decays. Two different \etmiss definitions are used. Calorimeter-based
\etmiss (\calomet) reconstructs the transverse energy in the event
with calibrated physics objects and the soft energy in the event is
reconstructed with calorimeter topoclusters. Track-based \etmiss (\trkmet), on
the other hand, uses tracks matched to the primary vertex to
reconstruct the soft energy, and is therefore less sensitive to
in-time pileup. Moreover, \calomet is a standardized
definition which is used across ATLAS analyses, while \trkmet uses
physics object definitions from analysis-specific optimizations,
yielding better scale and angular resolution than \calomet. 

Quality cuts on the tracks entering \trkmet are enumerated in
section~\ref{chap:reco:sec:met}. If a track fails this selection, but
is associated with either an electron or a muon, it is added to the
track collection. Electrons are required to satisfy either (1) {\it
  medium} identification requirements, $\et^{\textrm{cluster}} > 10
\gev$, and $|\eta| < 2.47$ or (2) the analysis-level electron
requirements described in
section~\ref{chap:analysis:sec:objects:subsec:electrons}. Muons are
required to satisfy cuts which are looser than those in the analysis,
namely combined muons with $\pt > 6 \gev$, $|\eta| < 2.5$ and
$|z_0\sin (\theta)| < 1$ mm. Tracks that overlap with reconstructed
electrons and muons are removed from the track collection, as are tracks
that fall within a cone of 0.4 of an analysis-level jet. This improves
the \etmiss resolution because neutral hadrons associated with jets which do
not produce tracks but do deposit energy in the calorimeter are not
excluded.

The systematic uncertainties from all of the reconstructed physics objects which
define \etmiss must be combined coherently to obtain the total
uncertainty on \etmiss. For the hard objects, such as leptons and
jets, the uncertainties are evaluated inpendently and propagated to
\etmiss. For the soft term, uncertainties associated with the scale
and resolution calibrations are evaluated with data-driven
techniques for both \calomet~\cite{bib:ATLAS-CONF-2013-082} and
\trkmet. For the calorimeter \etmiss uncertainties, a \Zmm~sample is
isolated, and to simplify \etmiss such that only muons and the soft
term contribute, exactly zero jets with \pt~greater than $20 \gev$ are
required. To evaluate
the scale uncertainty, the \etmiss vector is projected onto
$\vect{$p$}_{\mathrm{T}}$ of the $Z$ boson, and the mean value of this quantity is
evaluated in bins of $\sum{\et}$. The difference between data and MC
simulation is assigned as the scale uncertainty on the soft term,
which, for $\sqrts = 8 \tev$ data, is 3.6\% averaged over all
$\sum{\et}$. The soft term resolution uncertainty is determined by
comparing data and simulation, but the quantities of interest are the
widths of the $x$ and $y$ components of \etmiss. Integrating over
$\sum{\et}$, the mean uncertainty is 2.3\%. Another approach for
evaluating the uncertainty, which utilizes the balance between hard
and soft objects, yields similar results. 

The \trkmet uncertainties are evaluated with a data-driven approach that balances
the \pt~of hard objects and the soft term. Defining the energy
components of the hard objects as

\begin{equation}
E_{\mathrm {x(y)}}^{\mathrm {hard}} = \Sigma_{\mu}\, E_{\mathrm
  {x(y)}}^{\mu} + \Sigma_{ele}\, E_{\mathrm{x(y)}}^{ele} +
\Sigma_{\nu} \,  E_{\mathrm {x(y)}}^{\nu} + \Sigma_{\textrm{jets}}\,
E_{\mathrm {x(y)}}^{\textrm{jets}},
\end{equation}

\noindent
the neutrino term is set to zero in data and extracted from the truth
information in simulation. The soft term is then just the reconstructed
\trkmet with the hard objects subtracted. To evaluate the uncertainty,
the scale and resolutions of \ptsoft~are measured in data and
simulation as a function of $E_{\mathrm {T}}^{\mathrm {hard}}$ and the
average number of interactions per bunch crossing $\left \langle \mu
\right \rangle$. Binning in $E_{\mathrm {T}}^{\mathrm {hard}}$ allows
the uncertainty to be extrapolated to events which are kinematically
different from \Zmm~events, while separating the uncertainty into
$\left \langle \mu \right \rangle$ bins accounts for potential effects
from in-time pileup. 

The \ptsoft~term is decomposed into the component which is parallel to
the $E_{\mathrm {T}}^{\mathrm {hard}}$ and one which is perpindicular
to $E_{\mathrm {T}}^{\mathrm {hard}}$. The former is sensitive to the
modeling of the soft hadronic activity against which the hard system
is recoiling, while the latter is a measure the intrinsic \ptsoft
resolution. To extract the uncertainty, for the parallel component,
the simulation is smeared with a Gaussian to reproduce the resolution
in data and then shifted to the scale observed in data. These two
corrections are applied as systematic uncertainties. For the
perpindicular component, only smearing is performed. The resulting
uncertainties are summarized in table~\ref{tab:trkmet_uncert}. Very
little pile-up dependence has been observed, and therefore the
uncertainties are not ultimately assigned as a function of $\left
\langle \mu \right \rangle$. Moreover, it has been checked that these
uncertainties are robust against different MC generators as well as
the jet scale and resolution uncertainties, with the largest effect
coming from the parton shower model in simulation. Uncertainties are
adjusted accordingly to account for this. 

\begin{table}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{c|c|c|c}
$E_{\mathrm {T}}^{\mathrm {hard}}$ bin (\GeV) & scale (\GeV)  &
reso. para. (\GeV) & reso. perp. (\GeV)  \\ \hline
0-5     & 0.28 & 1.57 & 1.68 \\
5-10   & 0.38 & 1.60 & 1.58 \\
10-15 & 0.58 & 1.60 & 1.61 \\
15-20 & 0.73 & 1.75 & 1.72 \\
20-25 & 0.82 & 1.88 & 1.72 \\
25-30 & 0.99 & 2.13 & 1.84 \\
35-40 & 1.12 & 2.35 & 2.10 \\
40-45 & 1.20 & 2.58 & 2.15 \\
50+     & 1.44 & 3.33 & 2.68 \\
\end{tabular}
}
\caption{The absolute systematic variations (\gev) on the \ptsoft~term
  of \trkmet. Scale and
  resolution uncertainties are shown in bins of $E_{\mathrm
    {T}}^{\mathrm {hard}}$. The scale uncertainty corresponds to the
  shift factor which is applied to the parallel projection of
  \ptsoft~in simulation. The resolution uncertainties, split into the
  parallel and perpindicular components, correspond to additional
  smearing applied to the soft terms to match the width of the
  distribution observed in data.} 
\label{tab:trkmet_uncert}
\end{table}

